
#figure(
  caption: [
    *High-level architecture of the SmolVLA Vision Encoder Transformer block.* The design is composed of two primary sub-modules: the Multi-Head Attention mechanism (top) and the Multi-Layer Perceptron (MLP) with GELU activation (bottom).
  ],
  image(
    "model-overview.svg",
    width: 75%,
  ),
) <fig:model-overview>
