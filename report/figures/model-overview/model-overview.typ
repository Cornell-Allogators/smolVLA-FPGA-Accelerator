
#figure(
  caption: [
    *High-level architecture of the SmolVLA Transformer block.* The design is composed of two primary sub-modules: the Multi-Head Attention mechanism (top) and the Multi-Layer Perceptron (MLP) with GELU activation (bottom). The diagram illustrates the dataflow through the Linear projections, residual connections, and Layer Normalization steps that constitute a single layer.
  ],
  image(
    "model-overview.svg",
    width: 70%
  )
) <fig:model-overview>
