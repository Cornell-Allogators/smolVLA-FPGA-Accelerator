{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Roofline Modeling: Vision & Text Encoders\n",
    "\n",
    "This notebook provides a comprehensive roofline analysis for the **VLM portion** of smolVLA, specifically the **Vision Encoder** and **Text Encoder** (Language Model).\n",
    "\n",
    "**Source of Truth**: `model-preparation/full/tests/smolvla_test_vectors/model_shape.txt`\n",
    "\n",
    "We explore:\n",
    "1.  **Max Throughput**: Theoretical limits on the Alveo U280.\n",
    "2.  **Resource Constraints**: Memory usage and bandwidth bottlenecks.\n",
    "3.  **Quantization**: Impact of INT8 and INT4 precision on performance.\n",
    "4.  **Acceleration Ideas**: Strategies to improve performance beyond simple quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 150,\n",
    "    'lines.linewidth': 2\n",
    "})\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5bf90",
   "metadata": {},
   "source": [
    "## 1. Hardware Specifications (Alveo U280)\n",
    "\n",
    "We define the peak performance for different precisions. Note that INT8/INT4 performance is significantly higher on FPGAs due to DSP efficiency and logic-based compute.\n",
    "\n",
    "*   **Frequency**: 300 MHz\n",
    "*   **FP32 Peak**: 5.4 TFLOPs\n",
    "*   **INT8 Peak**: ~18.6 TOPS (DSP-based estimate)\n",
    "*   **INT4 Peak**: ~37.2 TOPS (Assumed 2x INT8 via logic/packing)\n",
    "*   **Memory Bandwidth**: 460 GB/s (Peak), 300 GB/s (Realistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907dafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ = 300e6\n",
    "BW_PEAK = 460e9\n",
    "BW_REAL = 300e9\n",
    "\n",
    "# Peak Compute (Ops/s)\n",
    "P_FP32 = 5.41e12\n",
    "P_BF16 = 5.41e12 # Assumed same as FP32 for DSPs without native BF16\n",
    "P_INT8 = 18.6e12\n",
    "P_INT4 = 37.2e12\n",
    "\n",
    "print(f\"FP32 Peak: {P_FP32 / 1e12:.2f} TFLOPs/s\")\n",
    "print(f\"INT8 Peak: {P_INT8 / 1e12:.2f} TOPS/s\")\n",
    "print(f\"INT4 Peak: {P_INT4 / 1e12:.2f} TOPS/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d89d10",
   "metadata": {},
   "source": [
    "## 2. Model Dimensions\n",
    "\n",
    "We extract dimensions from `model_weights.h` and `model_shape.txt`.\n",
    "\n",
    "### Vision Encoder (`vision_model`)\n",
    "*   Layers: 12\n",
    "*   Hidden Dim ($V_D$): 768\n",
    "*   FFN Dim ($V_{FFN}$): 3072\n",
    "*   Attention: Standard ($Q=K=V=768$)\n",
    "*   Patch Embed: 768, 3, 16, 16\n",
    "\n",
    "### Text Encoder (`text_model`)\n",
    "*   Layers: 16\n",
    "*   Hidden Dim ($T_D$): 960\n",
    "*   FFN Dim ($T_{FFN}$): 2560\n",
    "*   **Attention (GQA/MQA)**:\n",
    "    *   $Q_{dim} = 960$\n",
    "    *   $K_{dim} = 320$\n",
    "    *   $V_{dim} = 320$\n",
    "    *   $Out_{dim} = 960$\n",
    "\n",
    "### Connector\n",
    "*   Projection: 12288 -> 960\n",
    "\n",
    "### LM Head\n",
    "*   Vocab: 49280 -> 960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69027f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Encoder\n",
    "V_LAYERS = 12\n",
    "V_D = 768\n",
    "V_FFN = 3072\n",
    "V_ATTN_D = 768 # Q, K, V, Out all 768\n",
    "\n",
    "# Text Encoder\n",
    "T_LAYERS = 16\n",
    "T_D = 960\n",
    "T_FFN = 2560\n",
    "T_Q_D = 960\n",
    "T_K_D = 320\n",
    "T_V_D = 320\n",
    "T_OUT_D = 960\n",
    "\n",
    "# Connector\n",
    "CONN_IN = 12288\n",
    "CONN_OUT = 960\n",
    "\n",
    "# LM Head\n",
    "VOCAB = 49280\n",
    "\n",
    "# Batch Size\n",
    "B = 1\n",
    "S = 50 # Sequence Length (Text)\n",
    "S_V = 256 # Sequence Length (Vision, 16x16 patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c4d4c",
   "metadata": {},
   "source": [
    "## 3. Resource Constraints: Memory Usage\n",
    "\n",
    "We calculate the total memory required to store weights for each encoder under different precisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabd035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params_vision(layers, D, FFN):\n",
    "    # Per Layer:\n",
    "    # Attn: 4 * D * D (Q, K, V, Out) + 4 * D (Biases)\n",
    "    # MLP: 2 * D * FFN (FC1, FC2) + FFN + D (Biases)\n",
    "    # Norms: 4 * D (2 Layers * (Scale + Bias))\n",
    "    attn = 4 * (D * D + D)\n",
    "    mlp = 2 * (D * FFN) + FFN + D\n",
    "    norms = 4 * D\n",
    "    return layers * (attn + mlp + norms)\n",
    "\n",
    "def calc_params_text(layers, D, FFN, Q_D, K_D, V_D, Out_D):\n",
    "    # Per Layer:\n",
    "    # Attn: (Q_D*D + K_D*D + V_D*D + Out_D*D) + (Q_D + K_D + V_D + Out_D) (Biases assumed?)\n",
    "    # Note: model_shape.txt didn't explicitly show biases for text attn, but likely present.\n",
    "    # MLP: 3 * D * FFN (Down, Gate, Up) -> Wait, Down is FFN->D, Gate/Up are D->FFN\n",
    "    # Norms: 4 * D\n",
    "    attn_weights = (Q_D * D) + (K_D * D) + (V_D * D) + (Out_D * D)\n",
    "    mlp_weights = 3 * (D * FFN)\n",
    "    norms = 4 * D\n",
    "    return layers * (attn_weights + mlp_weights + norms)\n",
    "\n",
    "v_params = calc_params_vision(V_LAYERS, V_D, V_FFN)\n",
    "t_params = calc_params_text(T_LAYERS, T_D, T_FFN, T_Q_D, T_K_D, T_V_D, T_OUT_D)\n",
    "\n",
    "# Connector & Head\n",
    "conn_params = CONN_IN * CONN_OUT\n",
    "head_params = VOCAB * T_D\n",
    "\n",
    "total_params = v_params + t_params + conn_params + head_params\n",
    "\n",
    "print(f\"Vision Params: {v_params / 1e6:.2f} M\")\n",
    "print(f\"Text Params:   {t_params / 1e6:.2f} M\")\n",
    "print(f\"Connector:     {conn_params / 1e6:.2f} M\")\n",
    "print(f\"LM Head:       {head_params / 1e6:.2f} M\")\n",
    "print(f\"Total Params:  {total_params / 1e6:.2f} M\")\n",
    "\n",
    "precisions = {\n",
    "    'FP32': 4,\n",
    "    'BF16': 2,\n",
    "    'INT8': 1,\n",
    "    'INT4': 0.5\n",
    "}\n",
    "\n",
    "print(\"\\n--- Memory Usage (GB) ---\")\n",
    "for name, bytes_per_param in precisions.items():\n",
    "    mem_gb = total_params * bytes_per_param / 1e9\n",
    "    print(f\"{name}: {mem_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9331810",
   "metadata": {},
   "source": [
    "## 4. Throughput & Roofline Analysis\n",
    "\n",
    "We calculate the Operational Intensity (OI) for the dominant kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ffc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_oi_linear(M, K, N, dtype_bytes):\n",
    "    # Matrix Multiply: (M, K) x (K, N) -> (M, N)\n",
    "    flops = 2 * M * K * N\n",
    "    # Weights (K, N) + Input (M, K) + Output (M, N)\n",
    "    bytes_xfer = (K*N + M*K + M*N) * dtype_bytes\n",
    "    return flops / bytes_xfer\n",
    "\n",
    "metrics = {}\n",
    "for p_name, p_bytes in precisions.items():\n",
    "    metrics[p_name] = {}\n",
    "    \n",
    "    # --- Vision Encoder ---\n",
    "    # MLP FC1: (B*S_V) x V_D -> V_FFN\n",
    "    metrics[p_name]['Vis_MLP'] = calc_oi_linear(B*S_V, V_D, V_FFN, p_bytes)\n",
    "    \n",
    "    # Patch Embed (Conv2d -> Im2Col -> GEMM)\n",
    "    # In: 3 channels, Out: 768, Kernel: 16x16\n",
    "    # Equivalent GEMM: (N_Patches) x (3*16*16) x (768)\n",
    "    # M=1024 (Patches), K=3*16*16=768, N=768\n",
    "    metrics[p_name]['Vis_PatchEmbed'] = calc_oi_linear(1024, 3*16*16, 768, p_bytes)\n",
    "    \n",
    "    # --- Text Encoder ---\n",
    "    # MLP Gate/Up: (B*S) x T_D -> T_FFN\n",
    "    metrics[p_name]['Txt_MLP'] = calc_oi_linear(B*S, T_D, T_FFN, p_bytes)\n",
    "    \n",
    "    # Attention Q Proj: (B*S) x T_D -> T_Q_D\n",
    "    metrics[p_name]['Txt_Attn_Q'] = calc_oi_linear(B*S, T_D, T_Q_D, p_bytes)\n",
    "    \n",
    "    # Attention K Proj: (B*S) x T_D -> T_K_D (Smaller!)\n",
    "    metrics[p_name]['Txt_Attn_K'] = calc_oi_linear(B*S, T_D, T_K_D, p_bytes)\n",
    "    \n",
    "    # --- Connector ---\n",
    "    # (B*S_V) x 12288 -> 960 (Assuming flattened vision tokens?)\n",
    "    # Actually, if it's token-wise, it might be (B*S_V) x (Something) -> (Something)\n",
    "    # But the weight is [960, 12288]. This implies Input Dim = 12288.\n",
    "    # If Vision Out is 768, then 12288 / 768 = 16 tokens concatenated?\n",
    "    # Let's assume M=1 (Single large vector) or M=Sequence Length / 16.\n",
    "    # Let's assume M=1 for now (Global descriptor?)\n",
    "    metrics[p_name]['Connector'] = calc_oi_linear(1, 12288, 960, p_bytes)\n",
    "\n",
    "print(\"--- Operational Intensity (FLOPs/Byte) ---\")\n",
    "for p_name, vals in metrics.items():\n",
    "    print(f\"{p_name}:\")\n",
    "    for k, v in vals.items():\n",
    "        print(f\"  {k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46258de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roofline_base(ax, title):\n",
    "    x = np.logspace(-1, 3, 100)\n",
    "    \n",
    "    # Ceilings\n",
    "    ceilings = [\n",
    "        ('FP32/BF16', P_FP32, 'k-'),\n",
    "        ('INT8', P_INT8, 'b-'),\n",
    "        ('INT4', P_INT4, 'g-')\n",
    "    ]\n",
    "    \n",
    "    # Memory Walls\n",
    "    y_mem = BW_REAL * x\n",
    "    ax.loglog(x, y_mem, 'r--', label='Memory Wall (300 GB/s)')\n",
    "    \n",
    "    for name, peak, style in ceilings:\n",
    "        y = np.minimum(peak, y_mem)\n",
    "        ax.loglog(x, y, style, label=f'{name} Peak')\n",
    "    \n",
    "    ax.set_xlabel('Operational Intensity (Ops/Byte)')\n",
    "    ax.set_ylabel('Performance (Ops/s)')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "\n",
    "def plot_kernels(ax, kernel_names, metrics_dict):\n",
    "    markers = {'FP32': 'o', 'BF16': 's', 'INT8': '^', 'INT4': 'D'}\n",
    "    colors = {'FP32': 'black', 'BF16': 'orange', 'INT8': 'blue', 'INT4': 'green'}\n",
    "    \n",
    "    for p_name, vals in metrics_dict.items():\n",
    "        # Determine which peak applies\n",
    "        if p_name in ['FP32', 'BF16']: peak = P_FP32\n",
    "        elif p_name == 'INT8': peak = P_INT8\n",
    "        else: peak = P_INT4\n",
    "        \n",
    "        for k_name in kernel_names:\n",
    "            if k_name not in vals: continue\n",
    "            oi = vals[k_name]\n",
    "            perf = min(peak, BW_REAL * oi)\n",
    "            \n",
    "            ax.plot(oi, perf, marker=markers[p_name], color=colors[p_name], markersize=10)\n",
    "            if p_name == 'INT4': # Label only once\n",
    "                ax.text(oi, perf*1.3, k_name, fontsize=8, ha='center', rotation=45)\n",
    "\n",
    "def plot_vision_roofline():\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    plot_roofline_base(ax, 'Vision Encoder Roofline')\n",
    "    plot_kernels(ax, ['Vis_MLP', 'Vis_PatchEmbed', 'Connector'], metrics)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_text_roofline():\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    plot_roofline_base(ax, 'Text Encoder Roofline')\n",
    "    plot_kernels(ax, ['Txt_MLP', 'Txt_Attn_Q', 'Txt_Attn_K'], metrics)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting Vision Encoder...\")\n",
    "plot_vision_roofline()\n",
    "\n",
    "print(\"Plotting Text Encoder...\")\n",
    "plot_text_roofline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
